# Curso de Web Scraping: Extracci√≥n de Datos en la Web

## **Modulo II - HTML: Requests y BeautifulSoup**
- [Clase 4 Descargando una p√°gina web](#4-descargando-una-p√°gina-web)
- [Clase 5 Parseando HTML con BeautifulSoup](#5-parseando-html-con-beautifulSoup)



# 4. **Descargando una p√°gina web**

## Parsing Pagina12

En este m√≥dulo veremos c√≥mo utilizar las bibliotecas `requests` y `bs4` para programar scrapers de sitios HTML es decir, sitios estaticos. Nos propondremos armar un scraper de noticias del diario [www.pagina12.com.ar](http://www.pagina12.com.ar/)

Supongamos que queremos leer el diario por internet. Lo primero que hacemos es abrir el navegador, escribir la URL del diario y apretar `Enter` para que aparezca la p√°gina del diario. Lo que ocurre en el momento en el que apretamos `Enter` es lo siguiente:

1. El navegador env√≠a una solicitud a la URL pidi√©ndole informaci√≥n.
2. El servidor recibe la petici√≥n y procesa la respuesta.
3. El servidor env√≠a la respuesta a la IP de la cual recibi√≥ la solicitud.
4. Nuestro navegador recibe la respuesta y la muestra¬†**formateada**¬†en pantalla.

Para hacer un scraper debemos hacer un programa que replique este flujo de forma autom√°tica y sistem√°tica para luego extraer la informaci√≥n deseada de la respuesta. 

Utilizaremos¬†`requests`¬†para realizar peticiones y recibir las respuestas y¬†`bs4`¬†para¬†*parsear*¬†la respuesta y extraer la informaci√≥n.

Te dejo unos links que tal vez te sean de utilidad:

- [C√≥digos de status HTTP](https://developer.mozilla.org/es/docs/Web/HTTP/Status)
- [Documentaci√≥n de requests](https://requests.kennethreitz.org/en/master/)
- [Documentaci√≥n de bs4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

Antes de seguir necesitamos instalar las siguientes librer√≠as:

- jupyter
- requests
- bs4


> üí° iniciamos nuestro entorno virtual, e instalamos.


```python
# importamos la biblioteca requests
import requests

#guardamos la URL en una variable
url='https://www.pagina12.com.ar/'

# generamos una solicitud a nuestra URL y guardamos el resultado
p12 = requests.get(url)

# verificamos si salio todo bien
p12.status_code
    # Respuestas informativas (100‚Äì199),
    # Respuestas satisfactorias (200‚Äì299),
    # Redirecciones (300‚Äì399),
    # Errores de los clientes (400‚Äì499),
    # y errores de los servidores (500‚Äì599).

# ver el contenido
print(p12.text)

#Buscar un t√≠tulo en el texto
#Muchas veces la respuesta a la solicitud puede ser algo que no sea un texto: una imagen, un archivo de audio, un video, etc.
p12.content

#Analicemos otros elementos de la respuesta. Encabezados de la respuesta
#vamos a ver el encabezado de la respuesta
p12.headers

#encabezado de la solicitud que hacemos
# existen paginas webs que al detectar el user-agent con nombre por defecto, bloquean la respuesta, solucion enmascarar el nombre
p12.request.headers

#que metodos utilizamos, existen varios metodos PUT, DELETE etc 
#El contenido de la request que acabamos de hacer est√° avisando que estamos utilizando la biblioteca requests para python 
#y que no es un navegador convencional. Se puede modificar
p12.request.method

#volver a consultar la URL a quien hicimos la solicitud
# util en caso de que existan redireccionamiento (cuando accedemos a un sitio y este nos redirecciona a otro sitio)
p12.request.url

```

## Resumen

- p12 = requests.get(url) // realiza la solicitud.
- p12.status_code // muestra el c√≥digo de status HTTP de la respuesta.
- print(p12.text) // muestra el HTML de la p√°gina sin formatear.
- p12.content // muestra el contenido de la respuesta, puede ser bytes, im√°genes, audio.
- p12.headers // muestra el encabezado de la respuesta.
- p12.request.headers // muestra el encabezado de la solicitud. El contenido de esta request avisa al servidor que se est√° utilizando requests en python y que no es un navegador convencional. Puede ser modificado.
- p12-request.url // muestra la url a la que se le hizo la solicitud.

# 5. Parseando HTML con BeautifulSoup

Beautiful Soup es una librer√≠a Python que permite extraer informaci√≥n de contenido en formato HTML o XML. Para usarla, es necesario especificar un parser, que es responsable de transformar un documento HTML o XML en un √°rbol complejo de objetos Python. 

Esto permite, por ejemplo, que podamos interactuar con los elementos de una p√°gina web como si estuvi√©semos utilizando las herramientas del desarrollador de un navegador.

<img src="./img/m2c2-1.jpg"/>

Luego de una breve introducci√≥n continuamos con la clase. Bien, ya obtuvimos el c√≥digo HTML de la p√°gina en la clase anterior. 

En esta clase veremos c√≥mo extraer de √©l la informaci√≥n deseada.

```python
#Parseamos el codigo HTML
# esta libreria nos permite extraer la informacion que para nosotros es de interes
from bs4 import BeautifulSoup

# separamos el texto largo en pequenas partes para poder identificar
# s-> soup o sopa
s = BeautifulSoup(p12.text, 'lxml')

#para obtener/saber el tipo de dato de nuestra variable 
type(s)

#imprimir s con el metodo "prettify" de manera que tengas una nocion de como esta 
# estructurada la pagina y poder ver de manera jerarquica el DOM
print (s.prettify())

# Buscamos un elemento con el metodo find()
# BeatifulSoup nos devuelve el primer elemento que encuentra que coincida con el parametro
s.find('ul')

```

Primer ejercicio: obtener un listado de links a las distintas secciones del diario.

- DOM: estructura jer√°rquica, html
- Usar el inspector de elementos para ver d√≥nde se encuentra la informaci√≥n, ayudara a identificar los elementos que deseemos analizar.
- Ojo cuando la p√°gina es responsive, debemos tener en cuenta tanto el dise√±o escritorio como mobil

```python
# Buscamos elementos especificos agregando atributos -> attrs
# Al atributo le pasamos un diccionario {'nombre atributo':'valor que esperamos obtener'}
# utilizamos el metodo find_all() -> para traer TODOS los elementos que coincidan con el parametro que estamos pasando -> 'li'
s.find('ul',attrs={'class':'horizontal-list main-sections hide-on-dropdown'}).find_all('li')
#como resultado nos devuelve una lista []
```